{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# from dataload import create_dataloader, data_create  # , get_split_cifar100\n",
        "sys.path.append(os.path.abspath(\"dataload.ipynb\"))"
      ],
      "metadata": {
        "id": "cY6fiTgbJ8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HM9XtrH3I-OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Code for training PMNIST CL tasks - Pytorch has problems with gradient hooks, so the weights are manually compared with previous task\n",
        "Do not use for training time calculation'''\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import logging\n",
        "# Configure logger\n",
        "# logging.basicConfig(filename=\"test.log\", format='%(filename)s: %(message)s', filemode='w')\n",
        "\n",
        "# Setting threshold level\n",
        "logging.basicConfig()\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "logging.info(\"Caution: This is the root logger!\")\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from model import (ClassifierMLP, _prune)\n",
        "\n",
        "from dataload import create_dataloader, data_create  # , get_split_cifar100\n",
        "from res18 import AResNet18, CNet, ResNet18\n",
        "from util_res import (all_task_eval, change_weights, check_model_version, dataset_eval,\n",
        "                      learnable_weights, load_and_check, turn_off_adjx)\n",
        "\n",
        "gpu_boole = torch.cuda.is_available()\n",
        "\n",
        "class Myclass:\n",
        "  name='mlp_exp1'\n",
        "  batch_size=128\n",
        "  dataroot='../data/'\n",
        "  dataset='pmnist'\n",
        "  model='mlp'\n",
        "  n_class=10\n",
        "  tasks=4\n",
        "  im_size=28\n",
        "  cn=1\n",
        "  hidden_size=100\n",
        "  epochs=30\n",
        "  lr=1e-3\n",
        "  lr_adj=1e-2\n",
        "  decay=0\n",
        "  loss='ce'\n",
        "  optim='adam'\n",
        "  train_p=95.\n",
        "  wt_para=0.22\n",
        "  prune_para=0.999999\n",
        "  prune_epoch=30\n",
        "  load_model=True\n",
        "  restart_tsk=0\n",
        "  model_pth=''\n",
        "  turn_off=True\n",
        "  savename='pmnist_task_'\n",
        "  print_ev=10\n",
        "  workers=4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "args=Myclass()\n",
        "\n",
        "\n",
        "for tsktodo in range(0,args.tasks-1):\n",
        "  args.restart_tsk=tsktodo+1\n",
        "  args.model_pth=\"mlp_exp1pmnist_task_\"+str(tsktodo)+\".pt\"\n",
        "  if args.model == 'ares18':\n",
        "      net = AResNet18(num_classes=args.n_class, tasks=args.tasks+1, channels=args.cn)\n",
        "  elif args.model == 'res18':\n",
        "      net = ResNet18(num_classes=args.n_class, channels=args.cn)\n",
        "  elif args.model == 'cnet':\n",
        "      net = CNet(num_classes=args.n_class, channels=args.cn, tasks=args.tasks)\n",
        "  elif args.model == 'mlp':\n",
        "      net = ClassifierMLP(image_size = args.im_size, output_shape=args.n_class, tasks=args.tasks+2, \\\n",
        "          layer_size=args.hidden_size, bn_boole=True)\n",
        "  else:\n",
        "      raise Exception(\"model architecture not found\")\n",
        "  # print(list(net.named_buffers()))\n",
        "\n",
        "  if args.loss == 'ce':\n",
        "      loss_metric = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  if gpu_boole:\n",
        "      net = net.cuda()\n",
        "\n",
        "  if args.load_model:\n",
        "      ### load from saved_model and do the checks, etc. ####\n",
        "      net = load_and_check(args, net, loss_metric)\n",
        "      # args.lr=args.lr2\n",
        "\n",
        "  if args.lr_adj == -1:\n",
        "      args.lr_adj = args.lr\n",
        "  pruned = False\n",
        "\n",
        "  if args.optim == 'adam':\n",
        "      optimizer = torch.optim.Adam([\n",
        "                      {'params': (param for name, param in net.named_parameters() if 'adjx' not in name), 'lr':args.lr,'momentum':0},\n",
        "                      {'params': (param for name, param in net.named_parameters() if 'adjx' in name), 'lr':args.lr_adj,'momentum':0,'weight_decay':args.decay}\n",
        "                  ])\n",
        "  elif args.optim == 'sgd':\n",
        "      optimizer = torch.optim.SGD([\n",
        "                  {'params': (param for name, param in net.named_parameters() if 'adjx' not in name), 'lr':args.lr,'momentum':0.9,'weight_decay':5e-4},\n",
        "                  {'params': (param for name, param in net.named_parameters() if 'adjx' in name), 'lr':args.lr_adj,'momentum':0,'weight_decay':args.decay}\n",
        "              ])\n",
        "  else:\n",
        "      raise Exception(\"Unknown optimizer error.\")\n",
        "\n",
        "  j = args.restart_tsk\n",
        "  print(j)\n",
        "  if j>0:\n",
        "      \"\"\"Hooks are unstable - this is a slow alternative\"\"\"\n",
        "      free_wts = learnable_weights(net, args, verbose=True)\n",
        "      logging.info(\"\\n Parameters that are still trainable = {} %\".format(free_wts*100))\n",
        "\n",
        "      for ix in range(j):\n",
        "          \"\"\"Checking whether old adjx and fixed weights are preserved\"\"\"\n",
        "          error_signal = check_model_version(net,old_path='./{}{}{}.pt'.format(args.name,args.savename,j-1)\\\n",
        "              ,task=ix)\n",
        "          assert(error_signal==False)\n",
        "\n",
        "  if args.dataset == 'pmnist':\n",
        "      if args.restart_tsk==0:\n",
        "          permutations = [torch.Tensor(np.random.permutation(784).astype(np.float64)).long() for _ in range(args.tasks+1)]\n",
        "          torch.save(torch.stack(permutations),'permutations.pt')\n",
        "      else:\n",
        "          permutations = torch.load('./permutations.pt')\n",
        "      logging.info(\"Loading {} dataset\".format(args.dataset))\n",
        "      training, testing = data_create(args)\n",
        "      train_loader, test_loader = create_dataloader(training, testing, args)\n",
        "  elif args.dataset == 's-cifar100' or args.dataset == 'cifar100': #args.dataroot changes\n",
        "      logging.info(\"Loading {} dataset\".format(args.dataset))\n",
        "      train_loader, test_loader = get_split_cifar100(args, j+1, class_size=args.n_class)\n",
        "  else:\n",
        "      raise Exception(\"Dataset {} Error - not available\".format(args.dataset))\n",
        "\n",
        "\n",
        "  EPOCHS = list(range(args.epochs))\n",
        "  for epoch in EPOCHS:\n",
        "\n",
        "      t1 = time.time()\n",
        "\n",
        "      print(\"Task:\",j,\"- Epoch:\",epoch)\n",
        "      if j>0:\n",
        "          if logging.getLogger().getEffectiveLevel()<=20:\n",
        "              print(\"Test acc for all previous tasks:\")\n",
        "              test_acc, test_loss = all_task_eval(net, task=j-1, loss_metric=loss_metric, args=args)\n",
        "\n",
        "          for ix in range(j):\n",
        "              net = turn_off_adjx(net, ix, bn_off=True)\n",
        "\n",
        "      for i, (x,y) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # if j>0:\n",
        "          #     for iz, inx in enumerate(range(j*args.n_class, (j+1)*args.n_class)):\n",
        "          #         y[y==inx]=iz\n",
        "          if gpu_boole:\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "          if args.dataset=='pmnist':\n",
        "              x = x.view(-1,28*28)[:,permutations[j]]\n",
        "              y = y.view(-1)\n",
        "\n",
        "          outputs = net(x,task=j)\n",
        "          # outputs = net(x)\n",
        "          # print(outputs.sum())\n",
        "\n",
        "          loss = loss_metric(outputs,y)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if j>0:\n",
        "              # manually keeping weights same for old adjx - needs to be implemented through hooks\n",
        "              net=change_weights(net, old_model='./{}{}{}.pt'.format(args.name,args.savename,j-1), present_task=j, path=True)\n",
        "\n",
        "          del loss; del x; del y; del outputs\n",
        "\n",
        "      ###### Epoch Stats Printing ###########\n",
        "      if (epoch)%args.print_ev==0 or (args.epochs-epoch)<10:\n",
        "          train_acc, train_loss = dataset_eval(net, train_loader, loss_=loss_metric, args=args, print_txt='Training', verbose = 1, task = j)\n",
        "          if logging.getLogger().getEffectiveLevel() <=20:\n",
        "              test_acc, test_loss= dataset_eval(net, test_loader, loss_=loss_metric, args=args, print_txt='Testing', verbose = 1, task = j)\n",
        "          test_acc_true, test_loss_true= dataset_eval(net, test_loader, loss_=loss_metric, args=args, \\\n",
        "              print_txt='Testing(Binary Mapping)', verbose = 1, task = j, round_=True)\n",
        "          t2 = time.time()\n",
        "          logging.debug('Time left for task:',((t2-t1)/60)*(args.epochs-epoch),'minutes')\n",
        "          print()\n",
        "      ######################################\n",
        "\n",
        "      ############## Pruning ##############\n",
        "      if (epoch >= (args.epochs - args.prune_epoch) and args.epochs>args.prune_epoch):\n",
        "          logging.info(\"Relevance Mapping Pruning...\")\n",
        "          # _prune(net,task=j,prune_para=args.prune_para)\n",
        "          _prune(net,task=j,prune_para=args.prune_para, wt_sp=True, wt_para=args.wt_para)\n",
        "          pruned=True\n",
        "          if (epoch)%args.print_ev==0 or (args.epochs-epoch)<10:\n",
        "              if logging.getLogger().getEffectiveLevel()<=20:\n",
        "                  test_acc, test_loss= dataset_eval(net, test_loader, loss_=loss_metric, args=args, \\\n",
        "                      print_txt='Testing after mapping pruning', verbose = 1, task = j)\n",
        "                  test_acc_true, test_loss_true= dataset_eval(net, test_loader, loss_=loss_metric, args=args, \\\n",
        "                      print_txt='Testing(Binary) post-prune', verbose = 1, task = j, round_=True)\n",
        "          # net = turn_off_adjx(net, j, bn_off=True)\n",
        "          # change this accuracy for other tasks ##################\n",
        "          if args.train_p != -1:\n",
        "              if epoch==(args.epochs-11) and epoch<210: # add a argparse for this epoch limit\n",
        "                  if train_acc < args.train_p and args.lr>1e-5: # this value changes for different experiments/datasets/models\n",
        "                      logging.info(\"changing LR and adding a few epochs\")\n",
        "                      EPOCHS += list(range(args.epochs, args.epochs+29))\n",
        "                      args.prune_epoch += 29\n",
        "                      args.epochs += 29\n",
        "\n",
        "                      args.lr /= 10.\n",
        "                      for ip, g in enumerate(optimizer.param_groups):\n",
        "                          if ip==0: # first parameter group (non adjx)\n",
        "                              g['lr'] = args.lr\n",
        "                      logging.debug(optimizer)\n",
        "          print()\n",
        "\n",
        "  #######################################################\n",
        "  for ix in range(j+1):\n",
        "      net = turn_off_adjx(net, ix, bn_off=True) # turns off adjacency and BN for the task by requires_grad=False\n",
        "\n",
        "  print(\"--------------------------------\")\n",
        "  print(\"Test acc for all tasks:\")\n",
        "  if logging.getLogger().getEffectiveLevel()<=20:\n",
        "      test_acc, test_loss = all_task_eval(net, task=j, loss_metric=loss_metric, args=args)\n",
        "      logging.info(\"Test acc, Test loss:\",test_acc, test_loss)\n",
        "  test_acc, test_loss = all_task_eval(net, task=j, loss_metric=loss_metric, args=args, round=True)\n",
        "  print(\"Test acc, Test loss: (Binary Mapping)\",test_acc, test_loss)\n",
        "  print(\"--------------------------------\")\n",
        "  print()\n",
        "  for ix in range(j+1):\n",
        "      net = turn_off_adjx(net, ix, bn_off=True)\n",
        "  # if j == args.tasks-1:\n",
        "  logging.info(\"Saving model...\")\n",
        "  torch.save(net,'{}{}{}.pt'.format(args.name,args.savename,j))\n",
        "  if args.turn_off==True:\n",
        "      exit(0)\n"
      ],
      "metadata": {
        "id": "7Q11NTogJ5fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089185aa-2e30-47aa-95d1-d0c12aa87055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Task: 1 - Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 80.735 Loss: 0.004758233503003915\n",
            "Testing(Binary Mapping) Accuracy: 88.3 Loss: 0.003044848125427961\n",
            "\n",
            "Task: 1 - Epoch: 1\n",
            "Task: 1 - Epoch: 2\n",
            "Task: 1 - Epoch: 3\n",
            "Task: 1 - Epoch: 4\n",
            "Task: 1 - Epoch: 5\n",
            "Task: 1 - Epoch: 6\n",
            "Task: 1 - Epoch: 7\n",
            "Task: 1 - Epoch: 8\n",
            "Task: 1 - Epoch: 9\n",
            "Task: 1 - Epoch: 10\n",
            "Training Accuracy: 87.6 Loss: 0.0030093983272711437\n",
            "Testing(Binary Mapping) Accuracy: 92.84 Loss: 0.0017824942228384317\n",
            "\n",
            "Task: 1 - Epoch: 11\n",
            "Task: 1 - Epoch: 12\n",
            "Task: 1 - Epoch: 13\n",
            "Task: 1 - Epoch: 14\n",
            "Task: 1 - Epoch: 15\n",
            "Task: 1 - Epoch: 16\n",
            "Task: 1 - Epoch: 17\n",
            "Task: 1 - Epoch: 18\n",
            "Task: 1 - Epoch: 19\n",
            "Task: 1 - Epoch: 20\n",
            "Training Accuracy: 88.78666666666666 Loss: 0.0027609904987116653\n",
            "Testing(Binary Mapping) Accuracy: 93.75 Loss: 0.0015834198851138353\n",
            "\n",
            "Task: 1 - Epoch: 21\n",
            "Training Accuracy: 89.045 Loss: 0.0027077896813551584\n",
            "Testing(Binary Mapping) Accuracy: 93.63 Loss: 0.0015741609083488584\n",
            "\n",
            "Task: 1 - Epoch: 22\n",
            "Training Accuracy: 89.24 Loss: 0.002647690129528443\n",
            "Testing(Binary Mapping) Accuracy: 93.64 Loss: 0.0015688967072870583\n",
            "\n",
            "Task: 1 - Epoch: 23\n",
            "Training Accuracy: 89.07 Loss: 0.002676587196191152\n",
            "Testing(Binary Mapping) Accuracy: 93.77 Loss: 0.0015629131525754928\n",
            "\n",
            "Task: 1 - Epoch: 24\n",
            "Training Accuracy: 89.10833333333333 Loss: 0.002677836318810781\n",
            "Testing(Binary Mapping) Accuracy: 93.83 Loss: 0.00154219105723314\n",
            "\n",
            "Task: 1 - Epoch: 25\n",
            "Training Accuracy: 89.29666666666667 Loss: 0.0026416834046443303\n",
            "Testing(Binary Mapping) Accuracy: 93.93 Loss: 0.0015627188163343817\n",
            "\n",
            "Task: 1 - Epoch: 26\n",
            "Training Accuracy: 89.24 Loss: 0.002661900522808234\n",
            "Testing(Binary Mapping) Accuracy: 93.85 Loss: 0.0015404920156113803\n",
            "\n",
            "Task: 1 - Epoch: 27\n",
            "Training Accuracy: 89.47166666666666 Loss: 0.0026206045856078464\n",
            "Testing(Binary Mapping) Accuracy: 93.73 Loss: 0.001524899154389277\n",
            "\n",
            "Task: 1 - Epoch: 28\n",
            "Training Accuracy: 89.35333333333334 Loss: 0.0026097038224339483\n",
            "Testing(Binary Mapping) Accuracy: 93.86 Loss: 0.0015403873820323496\n",
            "\n",
            "Task: 1 - Epoch: 29\n",
            "Training Accuracy: 89.64666666666666 Loss: 0.002548306991904974\n",
            "Testing(Binary Mapping) Accuracy: 94.07 Loss: 0.0015105011015199125\n",
            "\n",
            "--------------------------------\n",
            "Test acc for all tasks:\n",
            "task0 Accuracy: 98.59 Loss: 0.0003312192783982027\n",
            "task1 Accuracy: 94.07 Loss: 0.0015105011015199125\n",
            "Test acc, Test loss: (Binary Mapping) 96.33 0.0009208601899590576\n",
            "--------------------------------\n",
            "\n",
            "2\n",
            "Task: 2 - Epoch: 0\n",
            "Training Accuracy: 80.37666666666667 Loss: 0.004810507129629453\n",
            "Testing(Binary Mapping) Accuracy: 87.27 Loss: 0.003230935523658991\n",
            "\n",
            "Task: 2 - Epoch: 1\n",
            "Task: 2 - Epoch: 2\n",
            "Task: 2 - Epoch: 3\n",
            "Task: 2 - Epoch: 4\n",
            "Task: 2 - Epoch: 5\n",
            "Task: 2 - Epoch: 6\n",
            "Task: 2 - Epoch: 7\n",
            "Task: 2 - Epoch: 8\n",
            "Task: 2 - Epoch: 9\n",
            "Task: 2 - Epoch: 10\n",
            "Training Accuracy: 87.52833333333334 Loss: 0.0030597209009031456\n",
            "Testing(Binary Mapping) Accuracy: 92.62 Loss: 0.0018923066133633257\n",
            "\n",
            "Task: 2 - Epoch: 11\n",
            "Task: 2 - Epoch: 12\n",
            "Task: 2 - Epoch: 13\n",
            "Task: 2 - Epoch: 14\n",
            "Task: 2 - Epoch: 15\n",
            "Task: 2 - Epoch: 16\n",
            "Task: 2 - Epoch: 17\n",
            "Task: 2 - Epoch: 18\n",
            "Task: 2 - Epoch: 19\n",
            "Task: 2 - Epoch: 20\n",
            "Training Accuracy: 88.85666666666667 Loss: 0.002702533449480931\n",
            "Testing(Binary Mapping) Accuracy: 93.58 Loss: 0.001648548666201532\n",
            "\n",
            "Task: 2 - Epoch: 21\n",
            "Training Accuracy: 89.025 Loss: 0.0026853511907160284\n",
            "Testing(Binary Mapping) Accuracy: 93.47 Loss: 0.001636239593848586\n",
            "\n",
            "Task: 2 - Epoch: 22\n",
            "Training Accuracy: 89.20666666666666 Loss: 0.0026495583072304726\n",
            "Testing(Binary Mapping) Accuracy: 93.37 Loss: 0.0016226914677768946\n",
            "\n",
            "Task: 2 - Epoch: 23\n",
            "Training Accuracy: 89.145 Loss: 0.0026584085630873837\n",
            "Testing(Binary Mapping) Accuracy: 93.53 Loss: 0.001610274824500084\n",
            "\n",
            "Task: 2 - Epoch: 24\n",
            "Training Accuracy: 89.12833333333333 Loss: 0.0026595328281323115\n",
            "Testing(Binary Mapping) Accuracy: 93.6 Loss: 0.0015782504616305233\n",
            "\n",
            "Task: 2 - Epoch: 25\n",
            "Training Accuracy: 89.46666666666667 Loss: 0.002588115862260262\n",
            "Testing(Binary Mapping) Accuracy: 93.62 Loss: 0.0015857856068760157\n",
            "\n",
            "Task: 2 - Epoch: 26\n",
            "Training Accuracy: 89.47333333333333 Loss: 0.0025658915547033152\n",
            "Testing(Binary Mapping) Accuracy: 93.66 Loss: 0.0015858392126858235\n",
            "\n",
            "Task: 2 - Epoch: 27\n",
            "Training Accuracy: 89.43666666666667 Loss: 0.0025848459124565124\n",
            "Testing(Binary Mapping) Accuracy: 93.83 Loss: 0.0015742964966222643\n",
            "\n",
            "Task: 2 - Epoch: 28\n",
            "Training Accuracy: 89.51166666666667 Loss: 0.0025852223061025143\n",
            "Testing(Binary Mapping) Accuracy: 93.77 Loss: 0.0015614801531657577\n",
            "\n",
            "Task: 2 - Epoch: 29\n",
            "Training Accuracy: 89.62333333333333 Loss: 0.002574485546598832\n",
            "Testing(Binary Mapping) Accuracy: 93.72 Loss: 0.0015583176374435424\n",
            "\n",
            "--------------------------------\n",
            "Test acc for all tasks:\n",
            "task0 Accuracy: 98.59 Loss: 0.0003312192783982027\n",
            "task1 Accuracy: 94.07 Loss: 0.0015105011015199125\n",
            "task2 Accuracy: 93.72 Loss: 0.0015583176374435424\n",
            "Test acc, Test loss: (Binary Mapping) 95.46 0.0011333460057872191\n",
            "--------------------------------\n",
            "\n",
            "3\n",
            "Task: 3 - Epoch: 0\n",
            "Training Accuracy: 80.02833333333334 Loss: 0.004884376627703507\n",
            "Testing(Binary Mapping) Accuracy: 87.55 Loss: 0.0031534154567867518\n",
            "\n",
            "Task: 3 - Epoch: 1\n",
            "Task: 3 - Epoch: 2\n",
            "Task: 3 - Epoch: 3\n",
            "Task: 3 - Epoch: 4\n",
            "Task: 3 - Epoch: 5\n",
            "Task: 3 - Epoch: 6\n",
            "Task: 3 - Epoch: 7\n",
            "Task: 3 - Epoch: 8\n",
            "Task: 3 - Epoch: 9\n",
            "Task: 3 - Epoch: 10\n",
            "Training Accuracy: 87.68 Loss: 0.0030239609124759832\n",
            "Testing(Binary Mapping) Accuracy: 92.91 Loss: 0.0018154724536463618\n",
            "\n",
            "Task: 3 - Epoch: 11\n",
            "Task: 3 - Epoch: 12\n",
            "Task: 3 - Epoch: 13\n",
            "Task: 3 - Epoch: 14\n",
            "Task: 3 - Epoch: 15\n",
            "Task: 3 - Epoch: 16\n",
            "Task: 3 - Epoch: 17\n",
            "Task: 3 - Epoch: 18\n",
            "Task: 3 - Epoch: 19\n",
            "Task: 3 - Epoch: 20\n",
            "Training Accuracy: 89.24833333333333 Loss: 0.002665723840892315\n",
            "Testing(Binary Mapping) Accuracy: 93.7 Loss: 0.0016272281734272837\n",
            "\n",
            "Task: 3 - Epoch: 21\n",
            "Training Accuracy: 89.515 Loss: 0.0025937208354473115\n",
            "Testing(Binary Mapping) Accuracy: 93.77 Loss: 0.0016039567640051245\n",
            "\n",
            "Task: 3 - Epoch: 22\n",
            "Training Accuracy: 89.36833333333334 Loss: 0.0026234863388041657\n",
            "Testing(Binary Mapping) Accuracy: 93.69 Loss: 0.00160983692035079\n",
            "\n",
            "Task: 3 - Epoch: 23\n",
            "Training Accuracy: 89.47 Loss: 0.0025943293042480944\n",
            "Testing(Binary Mapping) Accuracy: 93.75 Loss: 0.0016011698567308485\n",
            "\n",
            "Task: 3 - Epoch: 24\n",
            "Training Accuracy: 89.55 Loss: 0.002615747293581565\n",
            "Testing(Binary Mapping) Accuracy: 93.73 Loss: 0.0015989510157145559\n",
            "\n",
            "Task: 3 - Epoch: 25\n",
            "Training Accuracy: 89.62666666666667 Loss: 0.002572599550584952\n",
            "Testing(Binary Mapping) Accuracy: 93.85 Loss: 0.0015613715644925833\n",
            "\n",
            "Task: 3 - Epoch: 26\n",
            "Training Accuracy: 89.66 Loss: 0.0025536818981170654\n",
            "Testing(Binary Mapping) Accuracy: 93.89 Loss: 0.0015429876428097486\n",
            "\n",
            "Task: 3 - Epoch: 27\n",
            "Training Accuracy: 89.765 Loss: 0.002545465712994337\n",
            "Testing(Binary Mapping) Accuracy: 93.82 Loss: 0.0015600092488341033\n",
            "\n",
            "Task: 3 - Epoch: 28\n",
            "Training Accuracy: 89.60166666666667 Loss: 0.002569133186340332\n",
            "Testing(Binary Mapping) Accuracy: 93.87 Loss: 0.0015462022609077394\n",
            "\n",
            "Task: 3 - Epoch: 29\n",
            "Training Accuracy: 89.67333333333333 Loss: 0.002508847067505121\n",
            "Testing(Binary Mapping) Accuracy: 94.03 Loss: 0.001526137511152774\n",
            "\n",
            "--------------------------------\n",
            "Test acc for all tasks:\n",
            "task0 Accuracy: 98.59 Loss: 0.0003312192783982027\n",
            "task1 Accuracy: 94.07 Loss: 0.0015105011015199125\n",
            "task2 Accuracy: 93.72 Loss: 0.0015583176374435424\n",
            "task3 Accuracy: 94.03 Loss: 0.001526137511152774\n",
            "Test acc, Test loss: (Binary Mapping) 95.10249999999999 0.0012315438821286079\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}