{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U9aXv1hCkuh",
        "outputId": "3eb105c9-edf9-4032-a4ad-1b7e3662f402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIZKy86jDQBS",
        "outputId": "6361e2a1-8e00-4cf0-dbf8-317024ea352b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ContinualAI/colab.git continualai/colab\n",
        "from continualai.colab.scripts import mnist\n",
        "mnist.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prPPMvigDQw3",
        "outputId": "fbb5a463-bc5e-47f8-850d-fce19f12bb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'continualai/colab'...\n",
            "remote: Enumerating objects: 378, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 378 (delta 78), reused 64 (delta 62), pack-reused 258\u001b[K\n",
            "Receiving objects: 100% (378/378), 26.97 MiB | 19.23 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n",
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35802210.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 893871.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9151539.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11694615.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "\n",
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
        "torch.manual_seed(1);"
      ],
      "metadata": {
        "id": "j9IygfgwDY2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, t_train, x_test, t_test = mnist.load()"
      ],
      "metadata": {
        "id": "8959ufGnDoEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def permute_mnist(mnist, seed):\n",
        "    np.random.seed(seed)\n",
        "    print(\"starting permutation...\")\n",
        "    h = w = 28\n",
        "    perm_inds = list(range(h*w))\n",
        "    np.random.shuffle(perm_inds)\n",
        "    # print(perm_inds)\n",
        "    perm_mnist = []\n",
        "    for set in mnist:\n",
        "        num_img = set.shape[0]\n",
        "        flat_set = set.reshape(num_img, w * h)\n",
        "        perm_mnist.append(flat_set[:, perm_inds].reshape(num_img, 1, w, h))\n",
        "    print(\"done.\")\n",
        "    return perm_mnist"
      ],
      "metadata": {
        "id": "UXMjXZsaDbZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task 1\n",
        "task_1 = [(x_train, t_train), (x_test, t_test)]\n",
        "\n",
        "# task 2\n",
        "x_train2, x_test2 = permute_mnist([x_train, x_test], 1)\n",
        "task_2 = [(x_train2, t_train), (x_test2, t_test)]\n",
        "\n",
        "# task 3\n",
        "x_train3, x_test3 = permute_mnist([x_train, x_test], 2)\n",
        "task_3 = [(x_train3, t_train), (x_test3, t_test)]\n",
        "\n",
        "# task list\n",
        "\n",
        "# task 4\n",
        "x_train4, x_test4 = permute_mnist([x_train, x_test], 3)\n",
        "task_4 = [(x_train4, t_train), (x_test4, t_test)]\n",
        "\n",
        "# task 3\n",
        "x_train5, x_test5 = permute_mnist([x_train, x_test], 4)\n",
        "task_5 = [(x_train5, t_train), (x_test5, t_test)]\n",
        "\n",
        "tasks = [task_1, task_2, task_3,task_4,task_5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsjn06M7D927",
        "outputId": "96191eb2-cd2e-4f86-b5af-233e7b12ae15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting permutation...\n",
            "done.\n",
            "starting permutation...\n",
            "done.\n",
            "starting permutation...\n",
            "done.\n",
            "starting permutation...\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"VGG(\n",
        "  (convs): Sequential(\n",
        "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (2): ReLU()\n",
        "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (6): ReLU()\n",
        "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (10): ReLU()\n",
        "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (14): ReLU()\n",
        "    (15): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (18): ReLU()\n",
        "    (19): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (22): ReLU()\n",
        "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (26): ReLU()\n",
        "    (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (28): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (30): ReLU()\n",
        "    (31): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (34): ReLU()\n",
        "    (35): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (38): ReLU()\n",
        "    (39): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (42): ReLU()\n",
        "    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (46): ReLU()\n",
        "    (47): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (49): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (50): ReLU()\n",
        "    (51): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "  )\n",
        "  (fcs): Sequential(\n",
        "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
        "    (1): ReLU()\n",
        "    (2): Dropout(p=0.5, inplace=False)\n",
        "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "    (4): ReLU()\n",
        "    (5): Dropout(p=0.5, inplace=False)\n",
        "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
        "  )\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# VGG_NPF\n",
        "class VGGNEW(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.C11= nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.B11=nn.BatchNorm2d(64)\n",
        "\n",
        "\n",
        "        self.C12=nn.Conv2d(64, 64, kernel_size=(3, 3),padding=1)\n",
        "        self.B12=nn.BatchNorm2d(64)\n",
        "        self.M12=nn.AvgPool2d(2,2)\n",
        "\n",
        "\n",
        "        self.C21=nn.Conv2d(64, 128,kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B21=nn.BatchNorm2d(128)\n",
        "\n",
        "        self.C22=nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B22=nn.BatchNorm2d(128)\n",
        "        self.M22=nn.AvgPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C31=nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B31=nn.BatchNorm2d(256)\n",
        "\n",
        "\n",
        "        self.C32=nn.Conv2d(256, 256, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B32=nn.BatchNorm2d(256)\n",
        "\n",
        "        self.C33=nn.Conv2d(256, 256, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B33=nn.BatchNorm2d(256)\n",
        "        self.M33=nn.AvgPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C41=nn.Conv2d(256, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B41=nn.BatchNorm2d(512)\n",
        "\n",
        "        self.C42=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B42=nn.BatchNorm2d(512)\n",
        "\n",
        "\n",
        "        self.C43=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B43=nn.BatchNorm2d(512)\n",
        "        self.M43=nn.AvgPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C51=nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B51=nn.BatchNorm2d(512)\n",
        "\n",
        "        self.C52=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B52=nn.BatchNorm2d(512)\n",
        "\n",
        "        self.C53=nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B53=nn.BatchNorm2d(512)\n",
        "        self.M53=nn.AvgPool2d(2,2,padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, xb):\n",
        "        #out = xb.view(xb.size(0), -1)\n",
        "        self.Gate_NPF=[]\n",
        "        SIGMOID=torch.nn.Sigmoid()\n",
        "        beta=10\n",
        "\n",
        "        out = self.C11(xb)\n",
        "        out=self.B11(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C12(out)\n",
        "        out=self.B12(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "        out = self.M12(out)\n",
        "\n",
        "        out = self.C21(out)\n",
        "        out=self.B21(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C22(out)\n",
        "        out=self.B22(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "        out = self.M22(out)\n",
        "\n",
        "        out = self.C31(out)\n",
        "        out=self.B31(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C32(out)\n",
        "        out=self.B32(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C33(out)\n",
        "        out=self.B33(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "        out = self.M33(out)\n",
        "\n",
        "        out = self.C41(out)\n",
        "        out=self.B41(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C42(out)\n",
        "        out=self.B42(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C43(out)\n",
        "        out=self.B43(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "        out = self.M43(out)\n",
        "\n",
        "        out = self.C51(out)\n",
        "        out=self.B51(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C52(out)\n",
        "        out=self.B52(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "\n",
        "\n",
        "        out = self.C53(out)\n",
        "        out=self.B53(out)\n",
        "        self.Gate_NPF.append(SIGMOID(beta*out))\n",
        "        out = self.M53(out)\n",
        "\n",
        "    def Return_NPF_Gates(self):\n",
        "      return self.Gate_NPF"
      ],
      "metadata": {
        "id": "3gnCh7D-EVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGWeight(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.C11= nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.B11=nn.BatchNorm2d(64)\n",
        "\n",
        "\n",
        "        self.C12=nn.Conv2d(64, 64, kernel_size=(3, 3),padding=1)\n",
        "        self.B12=nn.BatchNorm2d(64)\n",
        "        self.M12=nn.MaxPool2d(2,2)\n",
        "\n",
        "\n",
        "        self.C21=nn.Conv2d(64, 128,kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B21=nn.BatchNorm2d(128)\n",
        "\n",
        "\n",
        "        self.C22=nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B22=nn.BatchNorm2d(128)\n",
        "        self.M22=nn.MaxPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C31=nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B31=nn.BatchNorm2d(256)\n",
        "\n",
        "\n",
        "        self.C32=nn.Conv2d(256, 256, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B32=nn.BatchNorm2d(256)\n",
        "\n",
        "\n",
        "        self.C33=nn.Conv2d(256, 256, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B33=nn.BatchNorm2d(256)\n",
        "        self.M33=nn.MaxPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C41=nn.Conv2d(256, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B41=nn.BatchNorm2d(512)\n",
        "\n",
        "\n",
        "        self.C42=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B42=nn.BatchNorm2d(512)\n",
        "\n",
        "\n",
        "        self.C43=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B43=nn.BatchNorm2d(512)\n",
        "        self.M43=nn.MaxPool2d(2,2,padding=1)\n",
        "\n",
        "        self.C51=nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B51=nn.BatchNorm2d(512)\n",
        "\n",
        "        self.C52=nn.Conv2d(512, 512, kernel_size=(3, 3),  padding=(1, 1))\n",
        "        self.B52=nn.BatchNorm2d(512)\n",
        "\n",
        "        self.C53=nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.B53=nn.BatchNorm2d(512)\n",
        "        self.M53=nn.MaxPool2d(2,2,padding=1)\n",
        "\n",
        "        self.FF=nn.Flatten()\n",
        "        self.L1=nn.Linear(in_features=2048, out_features=4096, bias=True)\n",
        "        self.D1=nn.Dropout(p=0.5, inplace=False)\n",
        "        self.L2=nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.D2=nn.Dropout(p=0.5, inplace=False)\n",
        "        self.L3=nn.Linear(in_features=4096, out_features=10, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, xb,G):\n",
        "       # xb = xb.view(xb.size(0), -1)\n",
        "        SIGMOID=torch.nn.Sigmoid()\n",
        "        beta=10\n",
        "        i=0\n",
        "        out = self.C11(xb)\n",
        "        out=self.B11(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "\n",
        "\n",
        "        out = self.C12(out)\n",
        "        out=self.B12(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "        out = self.M12(out)\n",
        "\n",
        "        out = self.C21(out)\n",
        "        out=self.B21(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "\n",
        "\n",
        "        out = self.C22(out)\n",
        "        out=self.B22(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "        out = self.M22(out)\n",
        "\n",
        "        out = self.C31(out)\n",
        "        out=self.B31(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "        out = self.C32(out)\n",
        "        out=self.B32(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "        out = self.C33(out)\n",
        "        out=self.B33(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "        out = self.M33(out)\n",
        "\n",
        "        out = self.C41(out)\n",
        "        out=self.B41(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "\n",
        "        out = self.C42(out)\n",
        "        out=self.B42(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "\n",
        "        out = self.C43(out)\n",
        "        out=self.B43(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "        out = self.M43(out)\n",
        "\n",
        "        out = self.C51(out)\n",
        "        out=self.B51(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "        out = self.C52(out)\n",
        "        out=self.B52(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "\n",
        "        out = self.C53(out)\n",
        "        out=self.B53(out)\n",
        "        out = out*G[i].detach()\n",
        "        i=i+1\n",
        "        out = self.M53(out)\n",
        "\n",
        "        out=self.FF(out)\n",
        "        out=self.L1(out)\n",
        "        out=self.D1(out)\n",
        "        out=self.L2(out)\n",
        "        out=self.D2(out)\n",
        "        out = self.L3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mKq20IxEgw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_NPF=VGGNEW()\n",
        "\n",
        "if train_on_gpu:\n",
        "  Model_NPF.cuda()\n",
        "\n",
        "# Model_NPV=VGGWeight()\n",
        "\n",
        "# if train_on_gpu:\n",
        "#   Model_NPV.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# for name, param in Model_NPF.named_parameters():\n",
        "#   print(name)\n",
        "# print(Model_NPV.C11.weight.lr)"
      ],
      "metadata": {
        "id": "jApsdyGeEwkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_dict = {}\n",
        "optpar_dict = {}\n",
        "ewc_lambda = 0.4\n",
        "model = VGGWeight().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "if train_on_gpu:\n",
        "  model.cuda()"
      ],
      "metadata": {
        "id": "Vo7n0htEE11b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(model, device, x_test, t_test):\n",
        "    # model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for start in range(0, len(t_test)-1, 256):\n",
        "      end = start + 256\n",
        "      with torch.no_grad():\n",
        "        x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        batch_images, labels = x.cuda(), y.cuda()\n",
        "        All_ones=torch.ones(batch_images.shape).cuda()\n",
        "        Model_NPF.forward(batch_images)\n",
        "        output=model.forward(All_ones,Model_NPF.Return_NPF_Gates())\n",
        "        # output = model(x)\n",
        "        test_loss += F.cross_entropy(output, y).item()\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    test_loss /= len(t_test)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(t_test),\n",
        "        100. * correct / len(t_test)))\n",
        "    return 100. * correct / len(t_test)"
      ],
      "metadata": {
        "id": "Gg8pagfUJy-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_task_update(task_id, x_mem, t_mem):\n",
        "\n",
        "  # model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # accumulating gradients\n",
        "  for start in range(0, len(t_mem)-1, 256):\n",
        "      end = start + 256\n",
        "      x, y = torch.from_numpy(x_mem[start:end]), torch.from_numpy(t_mem[start:end]).long()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      batch_images, labels = x.cuda(), y.cuda()\n",
        "      All_ones=torch.ones(batch_images.shape).cuda()\n",
        "      Model_NPF.forward(batch_images)\n",
        "      output=model.forward(All_ones,Model_NPF.Return_NPF_Gates())\n",
        "\n",
        "      # output = model(x)\n",
        "      loss = F.cross_entropy(output, y)\n",
        "      loss.backward()\n",
        "\n",
        "  fisher_dict[task_id] = {}\n",
        "  optpar_dict[task_id] = {}\n",
        "\n",
        "  # gradients accumulated can be used to calculate fisher\n",
        "  for name, param in model.named_parameters():\n",
        "\n",
        "    optpar_dict[task_id][name] = param.data.clone()\n",
        "    fisher_dict[task_id][name] = param.grad.data.clone().pow(2)"
      ],
      "metadata": {
        "id": "ccjebMnFE3m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"EWC\"\"\"\n",
        "\n",
        "\n",
        "def train_ewc(model, device, task_id, x_train, t_train, optimizer, epoch):\n",
        "    # model.train()\n",
        "\n",
        "    for start in range(0, len(t_train)-1, 256):\n",
        "      end = start + 256\n",
        "      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_images, labels = x.cuda(), y.cuda()\n",
        "      All_ones=torch.ones(batch_images.shape).cuda()\n",
        "      Model_NPF.forward(batch_images)\n",
        "      output=model.forward(All_ones,Model_NPF.Return_NPF_Gates())\n",
        "\n",
        "      # output = model(x)\n",
        "      loss = F.cross_entropy(output, y)\n",
        "\n",
        "      for task in range(task_id):\n",
        "        for name, param in model.named_parameters():\n",
        "          fisher = fisher_dict[task][name]\n",
        "          optpar = optpar_dict[task][name]\n",
        "          loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "id": "HSolWhzDE5Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ewc_accs = []\n",
        "for id, task in enumerate(tasks):\n",
        "  avg_acc = 0\n",
        "  print(\"Training on task-\",id)\n",
        "  (x_train, t_train), _ = task\n",
        "  for epoch in range(1, 10):\n",
        "    train_ewc(model, device, id, x_train, t_train, optimizer, epoch)\n",
        "  on_task_update(id, x_train, t_train)\n",
        "  for id_test, task in enumerate(tasks):\n",
        "    print(\"Testing on task-\",id_test)\n",
        "    _, (x_test, t_test) = task\n",
        "    acc = test(model, device, x_test, t_test)\n",
        "    avg_acc = avg_acc + acc\n",
        "  print(\"Avg acc: \", avg_acc / 5)\n",
        "  ewc_accs.append(avg_acc / 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFW6SiC6E6wb",
        "outputId": "5da5cfd6-3cc9-42b7-b934-99f43e61fa86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on task- 0\n",
            "Train Epoch: 1 \tLoss: 0.194600\n",
            "Train Epoch: 2 \tLoss: 0.110539\n",
            "Train Epoch: 3 \tLoss: 0.036785\n",
            "Train Epoch: 4 \tLoss: 0.004565\n",
            "Train Epoch: 5 \tLoss: 0.002601\n",
            "Train Epoch: 6 \tLoss: 0.001360\n",
            "Train Epoch: 7 \tLoss: 0.001427\n",
            "Train Epoch: 8 \tLoss: 0.001023\n",
            "Train Epoch: 9 \tLoss: 0.000750\n",
            "Testing on task- 0\n",
            "Test set: Average loss: 0.0002, Accuracy: 9839/10000 (98%)\n",
            "\n",
            "Testing on task- 1\n",
            "Test set: Average loss: 0.0110, Accuracy: 1365/10000 (14%)\n",
            "\n",
            "Testing on task- 2\n",
            "Test set: Average loss: 0.0104, Accuracy: 1664/10000 (17%)\n",
            "\n",
            "Testing on task- 3\n",
            "Test set: Average loss: 0.0111, Accuracy: 1633/10000 (16%)\n",
            "\n",
            "Testing on task- 4\n",
            "Test set: Average loss: 0.0102, Accuracy: 1793/10000 (18%)\n",
            "\n",
            "Avg acc:  32.588\n",
            "Training on task- 1\n",
            "Train Epoch: 1 \tLoss: 0.235404\n",
            "Train Epoch: 2 \tLoss: 0.045834\n",
            "Train Epoch: 3 \tLoss: 0.004400\n",
            "Train Epoch: 4 \tLoss: 0.002436\n",
            "Train Epoch: 5 \tLoss: 0.001498\n",
            "Train Epoch: 6 \tLoss: 0.000980\n",
            "Train Epoch: 7 \tLoss: 0.000710\n",
            "Train Epoch: 8 \tLoss: 0.000657\n",
            "Train Epoch: 9 \tLoss: 0.000611\n",
            "Testing on task- 0\n",
            "Test set: Average loss: 0.0005, Accuracy: 9674/10000 (97%)\n",
            "\n",
            "Testing on task- 1\n",
            "Test set: Average loss: 0.0006, Accuracy: 9539/10000 (95%)\n",
            "\n",
            "Testing on task- 2\n",
            "Test set: Average loss: 0.0157, Accuracy: 1304/10000 (13%)\n",
            "\n",
            "Testing on task- 3\n",
            "Test set: Average loss: 0.0161, Accuracy: 1388/10000 (14%)\n",
            "\n",
            "Testing on task- 4\n",
            "Test set: Average loss: 0.0158, Accuracy: 1046/10000 (10%)\n",
            "\n",
            "Avg acc:  45.902\n",
            "Training on task- 2\n",
            "Train Epoch: 1 \tLoss: 0.310499\n",
            "Train Epoch: 2 \tLoss: 0.011175\n",
            "Train Epoch: 3 \tLoss: 0.002469\n",
            "Train Epoch: 4 \tLoss: 0.001487\n",
            "Train Epoch: 5 \tLoss: 0.001298\n",
            "Train Epoch: 6 \tLoss: 0.000742\n",
            "Train Epoch: 7 \tLoss: 0.000580\n",
            "Train Epoch: 8 \tLoss: 0.000505\n",
            "Train Epoch: 9 \tLoss: 0.000477\n",
            "Testing on task- 0\n",
            "Test set: Average loss: 0.0016, Accuracy: 8900/10000 (89%)\n",
            "\n",
            "Testing on task- 1\n",
            "Test set: Average loss: 0.0012, Accuracy: 9150/10000 (92%)\n",
            "\n",
            "Testing on task- 2\n",
            "Test set: Average loss: 0.0006, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Testing on task- 3\n",
            "Test set: Average loss: 0.0189, Accuracy: 1212/10000 (12%)\n",
            "\n",
            "Testing on task- 4\n",
            "Test set: Average loss: 0.0190, Accuracy: 1139/10000 (11%)\n",
            "\n",
            "Avg acc:  59.96\n",
            "Training on task- 3\n",
            "Train Epoch: 1 \tLoss: 0.202559\n",
            "Train Epoch: 2 \tLoss: 0.007896\n",
            "Train Epoch: 3 \tLoss: 0.002626\n",
            "Train Epoch: 4 \tLoss: 0.000683\n",
            "Train Epoch: 5 \tLoss: 0.000455\n",
            "Train Epoch: 6 \tLoss: 0.000412\n",
            "Train Epoch: 7 \tLoss: 0.000401\n",
            "Train Epoch: 8 \tLoss: 0.000248\n",
            "Train Epoch: 9 \tLoss: 0.000246\n",
            "Testing on task- 0\n",
            "Test set: Average loss: 0.0027, Accuracy: 8420/10000 (84%)\n",
            "\n",
            "Testing on task- 1\n",
            "Test set: Average loss: 0.0025, Accuracy: 8342/10000 (83%)\n",
            "\n",
            "Testing on task- 2\n",
            "Test set: Average loss: 0.0010, Accuracy: 9254/10000 (93%)\n",
            "\n",
            "Testing on task- 3\n",
            "Test set: Average loss: 0.0006, Accuracy: 9557/10000 (96%)\n",
            "\n",
            "Testing on task- 4\n",
            "Test set: Average loss: 0.0222, Accuracy: 923/10000 (9%)\n",
            "\n",
            "Avg acc:  72.992\n",
            "Training on task- 4\n",
            "Train Epoch: 1 \tLoss: 0.267180\n",
            "Train Epoch: 2 \tLoss: 0.020305\n",
            "Train Epoch: 3 \tLoss: 0.001184\n",
            "Train Epoch: 4 \tLoss: 0.000945\n",
            "Train Epoch: 5 \tLoss: 0.000616\n",
            "Train Epoch: 6 \tLoss: 0.000435\n",
            "Train Epoch: 7 \tLoss: 0.000395\n",
            "Train Epoch: 8 \tLoss: 0.000393\n",
            "Train Epoch: 9 \tLoss: 0.000381\n",
            "Testing on task- 0\n",
            "Test set: Average loss: 0.0035, Accuracy: 7976/10000 (80%)\n",
            "\n",
            "Testing on task- 1\n",
            "Test set: Average loss: 0.0046, Accuracy: 6994/10000 (70%)\n",
            "\n",
            "Testing on task- 2\n",
            "Test set: Average loss: 0.0025, Accuracy: 8174/10000 (82%)\n",
            "\n",
            "Testing on task- 3\n",
            "Test set: Average loss: 0.0014, Accuracy: 9014/10000 (90%)\n",
            "\n",
            "Testing on task- 4\n",
            "Test set: Average loss: 0.0006, Accuracy: 9583/10000 (96%)\n",
            "\n",
            "Avg acc:  83.482\n"
          ]
        }
      ]
    }
  ]
}