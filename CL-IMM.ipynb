{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow\nprint(tensorflow.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-13T19:51:10.142391Z","iopub.execute_input":"2023-03-13T19:51:10.143953Z","iopub.status.idle":"2023-03-13T19:51:20.899773Z","shell.execute_reply.started":"2023-03-13T19:51:10.143887Z","shell.execute_reply":"2023-03-13T19:51:20.898236Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow==1.13.1 ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:51:24.862643Z","iopub.execute_input":"2023-03-13T19:51:24.863437Z","iopub.status.idle":"2023-03-13T19:52:15.116726Z","shell.execute_reply.started":"2023-03-13T19:51:24.863397Z","shell.execute_reply":"2023-03-13T19:52:15.114942Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow==1.13.1\n  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.4.0)\nCollecting keras-preprocessing>=1.0.5\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nCollecting keras-applications>=1.0.6\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (3.20.3)\nCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.6/367.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.21.6)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (2.2.0)\nRequirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.4.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.38.4)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.51.1)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1) (1.16.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.2.3)\nRequirement already satisfied: mock>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (5.0.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.11.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.1.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.11.0)\nInstalling collected packages: tensorflow-estimator, keras-preprocessing, astor, keras-applications, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.11.0\n    Uninstalling tensorflow-estimator-2.11.0:\n      Successfully uninstalled tensorflow-estimator-2.11.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.11.2\n    Uninstalling tensorboard-2.11.2:\n      Successfully uninstalled tensorboard-2.11.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.11.0\n    Uninstalling tensorflow-2.11.0:\n      Successfully uninstalled tensorflow-2.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\ntfx-bsl 1.12.0 requires tensorflow<3,>=2.11, but you have tensorflow 1.13.1 which is incompatible.\ntensorflow-transform 1.12.0 requires tensorflow<2.12,>=2.11.0, but you have tensorflow 1.13.1 which is incompatible.\ntensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 1.13.1 which is incompatible.\ntensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\ntensorflow-serving-api 2.11.0 requires tensorflow<3,>=2.11.0, but you have tensorflow 1.13.1 which is incompatible.\ntensorflow-decision-forests 1.2.0 requires tensorflow~=2.11.0, but you have tensorflow 1.13.1 which is incompatible.\ntensorflow-cloud 0.1.16 requires tensorboard>=2.3.0, but you have tensorboard 1.13.1 which is incompatible.\ntensorflow-cloud 0.1.16 requires tensorflow<3.0,>=1.15.0, but you have tensorflow 1.13.1 which is incompatible.\nexplainable-ai-sdk 1.3.3 requires tensorflow>=1.15.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed astor-0.8.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#####UTILS.Py####\nimport os\nimport time\nimport numpy as np\n\n\ndef SetDefaultAsNatural(FLAGS):\n    if hasattr(FLAGS, 'epoch') and FLAGS.epoch < 0:\n        FLAGS.epoch = 60\n\n    if hasattr(FLAGS, 'learning_rate') and FLAGS.learning_rate < 0:\n        FLAGS.learning_rate = 0.1\n\n    if hasattr(FLAGS, 'regularizer') and FLAGS.regularizer < 0:\n        FLAGS.regularizer = 1e-4\n\n    if hasattr(FLAGS, 'alpha') and FLAGS.alpha < 0:\n        FLAGS.alpha = 1.0 / 3\n\ndef PrintResults(alpha, results):\n\n    result_text = \"%.2f\" % alpha\n    for i in range(int(len(results)/2)):\n        result_text += \", train-idx%d: %.4f\" % (i+1, results[i])\n    for i in range(int(len(results)/2)):\n        result_text += \", test-idx%d: %.4f\" % (i+1, results[i + int(len(results)/2)])\n    print(result_text)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T17:10:28.094213Z","iopub.execute_input":"2023-03-13T17:10:28.094681Z","iopub.status.idle":"2023-03-13T17:10:28.105895Z","shell.execute_reply.started":"2023-03-13T17:10:28.094645Z","shell.execute_reply":"2023-03-13T17:10:28.104279Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport math\n\nclass Linear(object):\n    def __init__(self, h_in, out_len, trainable=True):\n        in_len = np.prod(h_in.get_shape().as_list()[1:])\n        initial = tf.truncated_normal([in_len, out_len], stddev = 0.1)\n        self.W = tf.Variable(initial, trainable=trainable)\n        self.b = tf.Variable(tf.constant(0.1, shape=[out_len]), trainable=trainable)\n        self.h_out = tf.matmul(tf.reshape(h_in, [-1,in_len]), self.W) + self.b\n\n\nclass RegLinear(Linear):\n    def __init__(self, Linear, FMo=None, trainable=False):\n        shape = Linear.W.get_shape().as_list()\n        self.W = tf.Variable(tf.constant(0.0, shape=shape), trainable=trainable)\n        self.b = tf.Variable(tf.constant(0.0, shape=[shape[-1]]), trainable=trainable)\n\n        if FMo == None:\n            self.reg_obj = tf.reduce_sum(tf.square(Linear.W - self.W))\n            self.reg_obj += tf.reduce_sum(tf.square(Linear.b - self.b)) \n        else:\n            self.reg_obj = tf.reduce_sum(tf.mul(FMo.W,tf.square(Linear.W - self.W)))\n            self.reg_obj += tf.reduce_sum(tf.mul(FMo.b,tf.square(Linear.b - self.b)))\n\n\nclass DropLinear():\n    def __init__(self, h_in, out_len, keep_prob, trainable=True):\n        in_len = np.prod(h_in.get_shape().as_list()[1:])\n        initial = tf.truncated_normal([in_len, out_len], stddev = 0.1)\n        self.W = tf.Variable(initial,trainable=trainable)\n        self.b = tf.Variable(tf.constant(0.1,shape=[out_len]),trainable=trainable)\n        \n        self.dropbase = RegLinear(self)\n\n        self.h_out = tf.nn.dropout(tf.matmul(h_in, self.W) + self.b, keep_prob)\\\n            + tf.matmul(h_in, self.dropbase.W) + self.dropbase.b","metadata":{"execution":{"iopub.status.busy":"2023-03-13T16:52:24.448149Z","iopub.execute_input":"2023-03-13T16:52:24.450295Z","iopub.status.idle":"2023-03-13T16:52:24.471964Z","shell.execute_reply.started":"2023-03-13T16:52:24.450215Z","shell.execute_reply":"2023-03-13T16:52:24.470765Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\n\nimport tensorflow as tf\n\ndef CopyLayers(sess, Lsc, Ltr = None, trainable_info=None):\n    if trainable_info == None:\n        trainable_info = np.tile([False], len(Lsc))\n\n    for l in range(len(Lsc)):\n        if len(Ltr) <= l:\n            Ltr.append(RegLinear(Lsc[l], trainable=trainable_info[l]))\n        sess.run(Ltr[l].W.assign(Lsc[l].W.eval(sess)))\n        sess.run(Ltr[l].b.assign(Lsc[l].b.eval(sess)))\n\n    return Ltr\n\ndef CopyLayerValues(sess, Lsc, Ltr = None):\n    if Ltr == None: Ltr = []\n    for l in range(len(Lsc)):\n        if len(Ltr) < l + 1: Ltr.append({})\n        Ltr[l]['W'] = Lsc[l].W.eval(sess)\n        Ltr[l]['b'] = Lsc[l].b.eval(sess)\n    return Ltr\n\ndef ZeroLayers(sess, L):\n    for l in range(len(L)):\n        shape = L[l].W.get_shape().as_list()\n        sess.run(L[l].W.assign(np.zeros(shape)))\n        sess.run(L[l].b.assign(np.zeros(shape[-1])))\n\ndef AddLayers(sess, L1, L2, Ltr):\n    # TODO: ad-hoc, it should be resolved in a natural way.\n    op = []\n    for l in range(len(L1)):\n        v = sess.run([L1[l].W, L2[l].W, L1[l].b, L2[l].b])\n        op += [Ltr[l].W.assign(v[0]+v[1]), Ltr[l].b.assign(v[2]+v[3])]\n    sess.run(op)\n\ndef AddMultiTaskLayers(sess, Ls, Ltr, Lw, noOfTask):\n    if len(Lw) < 1 or len(Ls) < 1:\n        return\n\n    ops = []\n    noOfLayer = len(Lw[0])\n\n    for i in range(noOfLayer):\n        val_W = 0\n        val_b = 0\n        for j in range(noOfTask):\n            val_W += Ls[j][i]['W'] * Lw[j][i]['W']\n            val_b += Ls[j][i]['b'] * Lw[j][i]['b']\n\n        ops.append(Ltr[i].W.assign(val_W))\n        ops.append(Ltr[i].b.assign(val_b))\n\n    sess.run(ops)\n\ndef Lw_maker(sess, L, alpha_info, adhoc_mnist=False):\n    Layers_out = [];\n    op = [];\n    for l in range(len(L)):\n        shape = L[l].W.get_shape().as_list()\n        rl = RegLinear(L[l])\n        op += [rl.W.assign(np.zeros(shape)+alpha_info[l])]\n        op += [rl.b.assign(np.zeros([shape[1],])+alpha_info[l])]\n        Layers_out.append(rl)\n    if adhoc_mnist: adhoc_MNIST_(sess,Layers_out)\n    sess.run(op)\n    return Layers_out\n\ndef CalculateWeighingBase(sess, L1, L2, alpha):\n    Lw = CopyLayers(sess, L1, [])\n    op = []\n    for l in range(len(L1)):\n        (W1, W2, b1, b2) = sess.run([L1[l].W, L2[l].W, L1[l].b, L2[l].b])\n        op += [Lw[l].W.assign( alpha*W2 / ((1-alpha)*W1 + alpha*W2)) ]\n        op += [Lw[l].b.assign( alpha*b2 / ((1-alpha)*b1 + alpha*b2)) ]\n    sess.run(op)\n    return Lw\n\ndef UpdateMultiTaskLwWithAlphas(L, alpha_list, noOfTask):\n    Lw = []\n\n    for i in range(noOfTask):\n        if len(Lw) < i + 1:\n            Lw.append([])\n        for l in range(len(L)):\n            if len(Lw[i]) < l + 1:\n                Lw[i].append({})\n            Lw[i][l]['W'] = np.zeros(L[l]['W'].shape) + alpha_list[i]\n            Lw[i][l]['b'] = np.zeros(L[l]['b'].shape) + alpha_list[i]\n    return Lw\n\ndef UpdateMultiTaskWeightWithAlphas(Fs, alpha_list, noOfTask):\n    Lw = []\n\n    total_W = 0\n    total_b = 0\n    noOfLayer = len(Fs[0])\n\n    # total: denominator\n    total_W = []\n    total_b = []\n    for i in range(noOfLayer):\n        total_W.append(alpha_list[noOfTask -1] * Fs[noOfTask - 1][i]['W'])\n        total_b.append(alpha_list[noOfTask -1] * Fs[noOfTask - 1][i]['b'])\n        for j in range(noOfTask - 1):\n            total_W[i] += alpha_list[j] * Fs[j][i]['W']\n            total_b[i] += alpha_list[j] * Fs[j][i]['b']\n\n    # calculating layer weight\n    for i in range(noOfTask):\n        if len(Lw) < i + 1:\n            Lw.append([])\n        for j in range(noOfLayer):\n            if len(Lw[i]) < j + 1:\n                Lw[i].append({})\n            val_W = alpha_list[i] * Fs[i][j]['W'] / total_W[j]\n            val_b = alpha_list[i] * Fs[i][j]['b'] / total_b[j]\n            Lw[i][j]['W'] = val_W\n            Lw[i][j]['b'] = val_b\n\n    return Lw\n\ndef PrintLayers(sess, L):\n    print(\"Welcome to my PrintLayers.\")\n    for l in range(len(L)):\n        shape = np.asarray(L[l].W.get_shape().as_list())\n        (W_, b_) = sess.run([L[l].W, L[l].b])\n        print(\"Level\", l+1, \":\", np.r_[shape, W_[0,0:2],W_[-1,-2:],b_[0],b_[-1]])\n    print(\"\")\n\ndef PrintLayers2(L):\n    print(\"Welcome to my PrintLayers.\")\n    for l in range(len(L)):\n        shape = L[l]['W'].shape\n        W_, b_ = [L[l]['W'], L[l]['b']]\n        print(\"Level\", l+1, \":\", np.r_[shape, W_[0,0:2],W_[-1,-2:],b_[0],b_[-1]])\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2023-03-13T16:40:10.388044Z","iopub.execute_input":"2023-03-13T16:40:10.388501Z","iopub.status.idle":"2023-03-13T16:40:10.430874Z","shell.execute_reply.started":"2023-03-13T16:40:10.388466Z","shell.execute_reply":"2023-03-13T16:40:10.429102Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"###MNIST.py####\nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# import utils\n# from model import imm\n\n\ndef XycPackage():\n    \"\"\"\n    Load Dataset and set package.\n    \"\"\"\n    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n    noOfTask = 3\n    x = []\n    x_ = []\n    y = []\n    y_ = []\n    xyc_info = []\n\n    x.append(np.concatenate((mnist.train.images,mnist.validation.images)))\n    y.append(np.concatenate((mnist.train.labels,mnist.validation.labels)))\n    x_.append(mnist.test.images)\n    y_.append(mnist.test.labels)\n    xyc_info.append([x[0], y[0], 'train-idx1'])\n\n    for i in range(1, noOfTask):\n        idx = np.arange(784)                 # indices of shuffling image\n        np.random.shuffle(idx)\n        \n        x.append(x[0].copy())\n        x_.append(x_[0].copy())\n        y.append(y[0].copy())\n        y_.append(y_[0].copy())\n\n        x[i] = x[i][:,idx]           # applying to shuffle\n        x_[i] = x_[i][:,idx]\n\n        xyc_info.append([x[i], y[i], 'train-idx%d' % (i+1)])\n\n    for i in range(noOfTask):\n        xyc_info.append([x_[i], y_[i], 'test-idx%d' % (i+1)])\n\n    return x, y, x_, y_, xyc_info","metadata":{"execution":{"iopub.status.busy":"2023-03-13T16:40:16.854320Z","iopub.execute_input":"2023-03-13T16:40:16.854860Z","iopub.status.idle":"2023-03-13T16:40:16.870012Z","shell.execute_reply.started":"2023-03-13T16:40:16.854815Z","shell.execute_reply":"2023-03-13T16:40:16.868257Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport math\n\n# from Linear import Linear, RegLinear, DropLinear\n\n\n#########################################################################################\n\nclass TransferNN(object):\n    def __init__(self, node_info, optim=('Adam',1e-4), name='[tf/NN]', trainable_info=None, keep_prob_info=None):\n\n        self.name = name\n        self.optim = optim\n        self.node_info = node_info\n\n        if keep_prob_info == None:\n            keep_prob_info = [0.8] + [0.5] * (len(node_info) - 2)\n        self.keep_prob_info = np.array(keep_prob_info)\n        self.eval_keep_prob_info = np.array([1.0] * (len(node_info) - 1))\n\n        if trainable_info == None:\n            trainable_info = [True] * len(node_info)\n        self.trainable_info = trainable_info\n\n        self.x = tf.placeholder(tf.float32, shape=[None, np.prod(node_info[0])])\n        self.y_ = tf.placeholder(tf.float32, shape=[None, node_info[-1]])\n        self.drop_rate = tf.placeholder(tf.float32, shape=[len(node_info) - 1])\n\n        self._BuildModel()\n        self._CrossEntropyPackage(optim)\n\n\n    def _BuildModel(self):\n        h_out_prev = tf.nn.dropout(self.x, self.drop_rate[0])\n\n        self.Layers = []\n        self.Layers_dropbase = []\n        for l in range(1, len(self.node_info)-1):\n            self.Layers.append(DropLinear(h_out_prev, self.node_info[l], self.drop_rate[l]))\n            self.Layers_dropbase.append(self.Layers[-1].dropbase)\n            \n            h_out_prev = tf.nn.relu(self.Layers[-1].h_out)\n\n        self.Layers.append(DropLinear(h_out_prev, self.node_info[-1], 1.0))\n        self.Layers_dropbase.append(self.Layers[-1].dropbase)\n        self.y = self.Layers[-1].h_out\n\n    def _OptimizerPackage(self, obj, optim):\n        if optim[0] == 'Adam': return tf.train.AdamOptimizer(optim[1]).minimize(obj)\n        elif optim[0] == 'SGD': return tf.train.GradientDescentOptimizer(optim[1]).minimize(obj)\n        elif optim[0] == 'Momentum': return tf.train.MomentumOptimizer(optim[1][0],optim[1][1]).minimize(obj)\n\n    def _CrossEntropyPackage(self, optim):\n        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=self.y))\n        self.train_step = self._OptimizerPackage(self.cross_entropy, optim) \n        self.correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(self.y_,1))\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n\n\n    def RegPatch(self, delta):\n        self.reg_obj = 0\n        self.Layers_reg = []\n\n        for l in range(0,len(self.Layers)):\n            self.Layers_reg.append(RegLinear(self.Layers[l]))\n            self.reg_obj += delta * self.Layers_reg[l].reg_obj\n\n        cel = tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=self.y)\n        self.cross_entropy = tf.reduce_mean(cel) + self.reg_obj\n        self.train_step = self._OptimizerPackage(self.cross_entropy, self.optim)\n\n    def CalculateFisherMatrix(self, sess, x, y, mb=1000):\n        FM = []\n        data_size = x.shape[0]\n        total_step = int(math.ceil(float(data_size)/mb))\n\n        for step in range(total_step):\n            ist = (step * mb) % data_size\n            ied = min(ist + mb, data_size)\n            y_sample = tf.reshape(tf.one_hot(tf.multinomial(self.y, 1), 10), [-1, 10])\n            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_sample, logits=self.y))\n            for l in range(len(self.Layers)):\n                if len(FM) < l + 1:\n                    FM.append({})\n                    FM[l]['W'] = np.zeros(self.Layers[l].W.get_shape().as_list())\n                    FM[l]['b'] = np.zeros(self.Layers[l].b.get_shape().as_list())\n                W_grad = tf.reduce_sum(tf.square(tf.gradients(cross_entropy,[self.Layers[l].W])), 0)\n                b_grad = tf.reduce_sum(tf.square(tf.gradients(cross_entropy,[self.Layers[l].b])), 0)\n                W_grad_val, b_grad_val = sess.run([W_grad, b_grad],\n                                    feed_dict={ self.x:x[ist:ied],\n                                                self.drop_rate:self.eval_keep_prob_info})\n                FM[l]['W'] += W_grad_val\n                FM[l]['b'] += b_grad_val\n\n        for l in range(len(self.Layers)):\n            FM[l]['W'] += 1e-8\n            FM[l]['b'] += 1e-8\n        \n        return FM\n\n    def Train(self, sess, x, y, x_, y_, epoch, mb=50):\n        data_size = x.shape[0]\n        total_step = int(math.ceil(float(data_size)/mb))\n\n        for e in range(epoch):\n            train_acc = 0\n            for step in range(total_step):\n                ist = (step * mb) % data_size\n                ied = min(ist + mb, data_size)\n\n                _, acc = sess.run([self.train_step, self.accuracy], \n                                    feed_dict={ self.x:x[ist:ied],\n                                                self.y_:y[ist:ied],\n                                                self.drop_rate:self.keep_prob_info})\n                train_acc += (ied - ist) * acc\n            train_acc /= data_size\n\n            test_acc = self.Test(sess, [[x_,y_,\"\"]], 1000, False)[0]\n            print(\"(%d, %d, %d, %.4f, %.4f)\" % (e+1, (e+1)*total_step,\n                (e+1)*data_size, train_acc, test_acc))\n\n    def Test(self, sess, xyc_info, mb=1000, debug=True): #ti: triple_info\n        acc_ret = []\n\n        for l in range(len(xyc_info)):\n            x_, y_, c = xyc_info[l]\n            comment = self.name + c\n            acc = self._Test(sess, x_, y_, mb)\n            acc_ret.append(acc)\n\n            if debug: \n                print(\"%s accuracy : %.4f\" % (comment, acc))\n\n        return acc_ret\n\n    def _Test(self, sess, x_, y_, mb):\n        acc = 0\n        data_size = x_.shape[0]\n        for step in range(int(math.ceil(float(data_size)/mb))):\n            ist = (step * mb) % data_size\n            ied = min(ist + mb, data_size)\n            acc += (ied - ist) * sess.run(self.accuracy, \n                                feed_dict={ self.x:x_[ist:ied], \n                                            self.y_:y_[ist:ied], \n                                            self.drop_rate:self.eval_keep_prob_info})\n        acc /= data_size\n        return acc\n\n\n    def TestTasks(self, sess, x, y, x_, y_, mb=1000, debug=True): #ti: triple_info\n        \"\"\"\n        test tasks using x, y, x_, y_ data.\n        Args:\n            x: list of original and shuffled input training data\n            y: label of training image\n            x_: list of original and shuffled input test data\n                (the size should be same with the size of x)\n            y_: label of test image\n        Returns:\n            ret: list of accuracy\n                [training_accuracies, ..., test_accuracies, ...]\n        \"\"\"\n        xyc_info = []\n        for i in range(len(x)):\n            xyc_info.append([x[i], y[i], 'train-idx%d' % i])\n        for i in range(len(x_)):\n            xyc_info.append([x_[i], y_[i], 'test-idx%d' % i])\n     \n        return self.Test(sess, xyc_info, mb=mb, debug=debug)\n\n    def TestAllTasks(self, sess, x_tasks, y_tasks, mb=1000, debug=True): #ti: triple_info\n        acc_ret = []\n        for l in range(len(x_tasks)):\n            x_ = x_tasks[l]\n            y_ = y_tasks[l]\n            acc = self._Test(sess, x_, y_, mb)\n            acc_ret.append(acc)\n        print(acc_ret)\n        if debug: \n            print(\"%s all test accuracy : %.4f\" % (self.name, np.average(acc_ret)))\n\n        return acc_ret","metadata":{"execution":{"iopub.status.busy":"2023-03-13T16:52:28.725122Z","iopub.execute_input":"2023-03-13T16:52:28.725531Z","iopub.status.idle":"2023-03-13T16:52:28.770325Z","shell.execute_reply.started":"2023-03-13T16:52:28.725496Z","shell.execute_reply":"2023-03-13T16:52:28.767888Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import time\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\n\nmean_imm = True\nmode_imm = False\nalpha = 0.8\noptimizer = 'SGD'\nlearning_rate = 1e-3\nepoch = 3\nbatch_size = 50\n\nno_of_task = 3\nno_of_node = [784,800,800,10]\nkeep_prob_info = [0.8, 0.5, 0.5]\n\n\n# data preprocessing\nx, y, x_, y_, xyc_info = XycPackage()\n\nstart = time.time()\n\nwith tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n    mlp = TransferNN(no_of_node, (optimizer, learning_rate), keep_prob_info=keep_prob_info)\n\n    sess.run(tf.global_variables_initializer())\n\n    L_copy = []\n    FM = []\n    for i in range(no_of_task):\n        print(\"\")\n        print(\"================= Train task #%d (%s) ================\" % (i+1, optimizer))\n\n        mlp.Train(sess, x[i], y[i], x_[i], y_[i], epoch, mb=batch_size)\n        mlp.Test(sess, [[x[i],y[i],\" train\"], [x_[i],y_[i],\" test\"]])\n\n        if mean_imm or mode_imm:\n            L_copy.append(CopyLayerValues(sess, mlp.Layers))\n        if mode_imm:\n            FM.append(CalculateFisherMatrix(sess, x[i], y[i]))\n\n    mlp.TestAllTasks(sess, x_, y_)\n\n\n    alpha_list = [(1-alpha)/(no_of_task-1)] * (no_of_task-1)\n    alpha_list.append(alpha)\n    ######################### Mean-IMM ##########################\n    if mean_imm:\n        print(\"\")\n        print(\"Main experiment on %s + Mean-IMM, shuffled MNIST\" % optimizer)\n        print(\"============== Train task #%d (Mean-IMM) ==============\" % no_of_task)\n\n        LW = UpdateMultiTaskLwWithAlphas(L_copy[0], alpha_list, no_of_task)\n        AddMultiTaskLayers(sess, L_copy, mlp.Layers, LW, no_of_task)\n        ret = mlp.TestTasks(sess, x, y, x_, y_, debug = False)\n        PrintResults(alpha, ret)\n\n        mlp.TestAllTasks(sess, x_, y_)\n\n    ######################### Mode-IMM ##########################\n    if mode_imm:\n        print(\"\")\n        print(\"Main experiment on %s + Mode-IMM, shuffled MNIST\" % optimizer)\n        print(\"============== Train task #%d (Mode-IMM) ==============\" % no_of_task)\n\n        LW = UpdateMultiTaskWeightWithAlphas(FM, alpha_list, no_of_task)\n        AddMultiTaskLayers(sess, L_copy, mlp.Layers, LW, no_of_task)\n        ret = mlp.TestTasks(sess, x, y, x_, y_, debug = False)\n        PrintResults(alpha, ret)\n\n        mlp.TestAllTasks(sess, x_, y_)\n\n    print(\"\")\n    print(\"Time: %.4f s\" % (time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2023-03-13T17:10:35.237454Z","iopub.execute_input":"2023-03-13T17:10:35.237899Z","iopub.status.idle":"2023-03-13T17:13:13.436637Z","shell.execute_reply.started":"2023-03-13T17:10:35.237863Z","shell.execute_reply":"2023-03-13T17:13:13.434388Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n\n================= Train task #1 (SGD) ================\n(1, 1200, 60000, 0.0986, 0.0980)\n(2, 2400, 120000, 0.0987, 0.0980)\n[tf/NN] train accuracy : 0.0987\n[tf/NN] test accuracy : 0.0980\n\n================= Train task #2 (SGD) ================\n(1, 1200, 60000, 0.0987, 0.0980)\n(2, 2400, 120000, 0.0987, 0.0980)\n[tf/NN] train accuracy : 0.0987\n[tf/NN] test accuracy : 0.0980\n\n================= Train task #3 (SGD) ================\n(1, 1200, 60000, 0.0987, 0.0980)\n(2, 2400, 120000, 0.0987, 0.0980)\n[tf/NN] train accuracy : 0.0987\n[tf/NN] test accuracy : 0.0980\n[0.09800000116229057, 0.09800000116229057, 0.09800000116229057]\n[tf/NN] all test accuracy : 0.0980\n\nMain experiment on SGD + Mean-IMM, shuffled MNIST\n============== Train task #3 (Mean-IMM) ==============\n-1.00, train-idx1: 0.0987, train-idx2: 0.0987, train-idx3: 0.0987, test-idx1: 0.0980, test-idx2: 0.0980, test-idx3: 0.0980\n[0.09800000116229057, 0.09800000116229057, 0.09800000116229057]\n[tf/NN] all test accuracy : 0.0980\n\nTime: 156.3151 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}